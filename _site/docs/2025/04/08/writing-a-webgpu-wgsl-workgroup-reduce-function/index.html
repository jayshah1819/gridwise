<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Abstraction Challenges in Writing a WebGPU/WGSL Workgroup Reduce Function | Gridwise</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Abstraction Challenges in Writing a WebGPU/WGSL Workgroup Reduce Function" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explore the challenges of writing generic WGSL library functions, including builtin declarations and workgroup memory limitations." />
<meta property="og:description" content="Explore the challenges of writing generic WGSL library functions, including builtin declarations and workgroup memory limitations." />
<link rel="canonical" href="http://localhost:4000/gridwise/docs/2025/04/08/writing-a-webgpu-wgsl-workgroup-reduce-function/" />
<meta property="og:url" content="http://localhost:4000/gridwise/docs/2025/04/08/writing-a-webgpu-wgsl-workgroup-reduce-function/" />
<meta property="og:site_name" content="Gridwise" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-08T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Abstraction Challenges in Writing a WebGPU/WGSL Workgroup Reduce Function" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-08T00:00:00-04:00","datePublished":"2025-04-08T00:00:00-04:00","description":"Explore the challenges of writing generic WGSL library functions, including builtin declarations and workgroup memory limitations.","headline":"Abstraction Challenges in Writing a WebGPU/WGSL Workgroup Reduce Function","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/gridwise/docs/2025/04/08/writing-a-webgpu-wgsl-workgroup-reduce-function/"},"url":"http://localhost:4000/gridwise/docs/2025/04/08/writing-a-webgpu-wgsl-workgroup-reduce-function/"}</script>
<!-- End Jekyll SEO tag -->
<link id="main-stylesheet" rel="stylesheet" href="/gridwise/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/gridwise/feed.xml" title="Gridwise" />
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/gridwise/">Gridwise</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon"></span>
        </label>

        <div class="nav-items">
  <a class="nav-item" href="/gridwise/about/">About</a>
  <a class="nav-item" href="/gridwise/gridwise/architecture/">Gridwise Architecture</a>
  <a class="nav-item" href="/gridwise/gridwise/binop/">Gridwise&#39;s Binary Operator Class</a>
  <a class="nav-item" href="/gridwise/gridwise/buffer/">Gridwise&#39;s Buffer Class</a>
  <a class="nav-item" href="/gridwise/gridwise/builtins-strategy/">Gridwise WebGPU @builtins Strategy</a>
  <a class="nav-item" href="/gridwise/gridwise/">Gridwise</a>
  <a class="nav-item" href="/gridwise/gridwise/primitive-design/">Gridwise WebGPU Primitive Strategy wrt Subgroups</a>
  <a class="nav-item" href="/gridwise/gridwise/scan-and-reduce/">Gridwise Scan and Reduce</a>
  <a class="nav-item" href="/gridwise/gridwise/sort/">Gridwise Sort</a>
  <a class="nav-item" href="/gridwise/gridwise/subgroup-strategy/">Gridwise WebGPU Subgroup Emulation Strategy</a>
  <a class="nav-item" href="/gridwise/gridwise/timing-strategy/">Gridwise WebGPU Timing Strategy</a>
  <a class="nav-item" href="/gridwise/gridwise/webgpu-object-caching-strategy/">Gridwise WebGPU Object Caching Strategy</a>
  <a class="nav-item" href="/gridwise/gridwise/writing-a-webgpu-wgsl-workgroup-reduce-function/">Abstraction Challenges in Writing a WebGPU/WGSL Workgroup Reduce Function</a>
</div>

      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Abstraction Challenges in Writing a WebGPU/WGSL Workgroup Reduce Function</h1>
    <div class="post-meta">
      <time class="dt-published" datetime="2025-04-08T00:00:00-04:00" itemprop="datePublished">
        Apr 8, 2025
      </time>
    </div>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="summary-of-pain-points">Summary of Pain Points</h2>

<p>When writing a generic library function in WGSL, we identified several key difficulties:</p>

<ul>
  <li><strong>Explicit Built-in Declarations:</strong> WGSL’s requirement to explicitly declare <code class="language-plaintext highlighter-rouge">@builtins</code> complicates APIs.</li>
  <li><strong>Inconsistent Built-in Variants:</strong> Not all built-ins have both 3D and 1D variants.</li>
  <li><strong>Module-Scoped Workgroup Memory:</strong> Declaring workgroup memory only at the module scope harms modularity.</li>
  <li><strong>Lack of Generics:</strong> The absence of function overloading or templating makes it difficult to write generic APIs.</li>
</ul>

<p>These limitations lead to a choice between two library design patterns: a few complex kernels using template-literal string pasting, or many simple kernels using metaprogramming.</p>

<hr />

<h2 id="scenario-a-reduceworkgroup-function">Scenario: A reduceWorkgroup Function</h2>

<p>We will focus on creating a <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> function that will be incorporated into a primitive library. We expect external users will face a need to input a linear array of data and have each workgroup compute the sum of a sub-section of that data. Our implementation, as part of a library, will allow them to do so without concern for how the reduction is implemented.</p>

<p>The semantics of the function are as follows: consider a workgroup of N invocations (e.g., N = 128); workgroup W will compute the sum of <code class="language-plaintext highlighter-rouge">input[W*N:W*(N+1))</code>, and each invocation in the workgroup will receive this sum as the return value of a function call.</p>

<h3 id="desired-function-call-caller-wgsl-definition">Desired Function Call: Caller (WGSL Definition)</h3>

<p>Ideally, making the function call from within a WGSL kernel would be simple, with the minimal call looking something like</p>

<pre><code class="language-wgsl">...
val = reduceWorkgroup(ptr);
...
</code></pre>

<p>Here we assume that <code class="language-plaintext highlighter-rouge">ptr</code> is an address pointing to an item within a linear input array, and that the private variable <code class="language-plaintext highlighter-rouge">val</code> and the deferenced value at <code class="language-plaintext highlighter-rouge">ptr</code> have the same datatype. If the workgroup has <code class="language-plaintext highlighter-rouge">W</code> invocations, then <code class="language-plaintext highlighter-rouge">val</code>, for every invocation in workgroup <code class="language-plaintext highlighter-rouge">W</code>, will assume the value <code class="language-plaintext highlighter-rouge">reduce(ptr[W*N:W*(N+1)))</code>.</p>

<p>This interface does not specify the reduction operation (for instance, addition, max, or min), nor the datatype. Possible options to address the former include:</p>

<ul>
  <li>A separate reduction function for each operation (<code class="language-plaintext highlighter-rouge">reduceAddWorkgroup</code>, <code class="language-plaintext highlighter-rouge">reduceMinWorkgroup</code>, etc.)</li>
  <li>A default operation (e.g., <code class="language-plaintext highlighter-rouge">add</code>)</li>
  <li>Separately defining an operation (a “binop” == a monoid) that must be explicitly set by the caller and implicitly used by the function</li>
</ul>

<p>However, we do not further discuss this issue in this document.</p>

<h3 id="desired-function-call-callee">Desired Function Call: Callee</h3>

<p>We would also like the callee function/kernel to be as simple as possible. The minimum kernel that calls our function, and places the output into an output array, would look something like this:</p>

<pre><code class="language-wgsl">@compute @workgroup_size(128) fn reduceKernel(
  @builtin(local_invocation_index) lidx: u32,
  @builtin(workgroup_id) wgid: vec3u
) {
  let r = reduceWorkgroup(&amp;in);
  if (lidx == 0) {
    out[wgid.x] = r;
  }
}
</code></pre>

<h3 id="issues-with-the-above-kernel">Issues with the above kernel</h3>

<h4 id="builtins-must-be-explicitly-declared">Builtins must be explicitly declared</h4>

<p>The above kernel has 5 lines of code. 2 of them are declaring built-ins. WGSL requires these declarations, but other languages have made different choices. Why are variables like <code class="language-plaintext highlighter-rouge">local_invocation_index</code> and <code class="language-plaintext highlighter-rouge">workgroup_id</code> not available within a kernel without the need for these declarations?</p>

<p>Possible ways to remove the need for these declarations include</p>

<ul>
  <li>Making a builtin variable available within kernels (e.g., CUDA’s <code class="language-plaintext highlighter-rouge">threadIdx</code>)</li>
  <li>Making a function call that retrieves a builtin variable available within kernels (e.g., SYCL’s <code class="language-plaintext highlighter-rouge">get_id()</code>)</li>
</ul>

<h4 id="some-3d-builtins-lack-a-1d-variant">Some 3D builtins lack a 1D variant</h4>

<p>The use of <code class="language-plaintext highlighter-rouge">wgid</code> above reflects a common implementation pattern: we logically have a 1D data space (<code class="language-plaintext highlighter-rouge">in</code>, <code class="language-plaintext highlighter-rouge">out</code>) and thus want our workgroups to also be organized as a 1D data space. However, the only access we have to the workgroup id is through a 3D builtin. Above, we just assume that such a builtin only uses the <code class="language-plaintext highlighter-rouge">.x</code> part of the <code class="language-plaintext highlighter-rouge">workgroup_id</code> builtin, but this assumption is checked nowhere.</p>

<p>It would be convenient to have a 1D builtin equivalent for every 3D builtin (specifically, WGSL lacks <code class="language-plaintext highlighter-rouge">workgroup_index</code> and <code class="language-plaintext highlighter-rouge">global_invocation_index</code>).</p>

<h3 id="implementing-reduceworkgroup">Implementing reduceWorkgroup</h3>

<p>Now let’s turn to actually implementing the <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> function.</p>

<p>An initial, simple implementation of <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> might allocate an workgroup-memory variable and use <code class="language-plaintext highlighter-rouge">atomicAdd</code> to accumulate inputs into it. It might look like this:</p>

<pre><code class="language-wgsl">fn reduceWorkgroup(input: ptr&lt;storage, array&lt;u32&gt;, read&gt;) -&gt; u32 {
  atomicAdd(&amp;wg_temp[0], input[?]);
  workgroupBarrier();
  return atomicLoad(&amp;wg_temp[0]);
}
</code></pre>

<h4 id="leaky-abstraction-wg_temp">Leaky abstraction: <code class="language-plaintext highlighter-rouge">wg_temp</code></h4>

<p>The above code uses storage <code class="language-plaintext highlighter-rouge">wg_temp</code> located in workgroup memory. WGSL requires that variables in the workgroup address space be declared at the module scope, which is a significant limitation on modularity. Who declares this storage? How big is it? Why can’t I declare it within the <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> function?</p>

<p>We could declare the workgroup memory <code class="language-plaintext highlighter-rouge">wg_temp_reduceWorkgroup</code> at the module scope as follows:</p>

<pre><code class="language-wgsl">var&lt;workgroup&gt; wg_temp_reduceWorkgroup: array&lt;atomic&lt;u32&gt;, 1&gt;;
fn reduceWorkgroup(input: ptr&lt;storage, array&lt;u32&gt;, read&gt;,
                   gid: vec3u,
                  ) -&gt; u32 {
  atomicAdd(&amp;wg_temp_reduceWorkgroup[0], input[?]);
  workgroupBarrier();
  return atomicLoad(&amp;wg_temp_reduceWorkgroup[0]);
}
</code></pre>

<p>However, such a declaration by the writer of a library might have a name conflict for the workgroup memory <code class="language-plaintext highlighter-rouge">wg_temp_reduceWorkgroup</code>, either within the library or with a different allocation defined by the user of the library. Note that if we define many different <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> functions, for instance ones that are specialized to datatype and reduction operation, we also must declare a different workgroup memory variable for each of them. Then a compiler would have to identify which specialized functions are actually used and only instantiate their workgroup memory regions. Finally, while it is likely that a complex workload that used many different reduce functions from this library could potentially share the same workgroup memory region used for the accumulation, the above code would not allow this sharing.</p>

<p>We might consider a language change that allows workgroup memory to be declared within a function instead of at module scope. This potentially eliminates name conflicts. But the programmer still must account for the use of workgroup memory.</p>

<h4 id="specifying-the-input-region">Specifying the input region</h4>

<p>The above code has the line <code class="language-plaintext highlighter-rouge">atomicAdd(&amp;wg_temp[0], input[?])</code>. How do we fill in the <code class="language-plaintext highlighter-rouge">[?]</code>?</p>

<p>What we want is for each individual instance (thread) to fetch the input value that corresponds to its global instance id. Instance 0 fetches <code class="language-plaintext highlighter-rouge">input[0]</code>, instance 128 fetches <code class="language-plaintext highlighter-rouge">input[128]</code>, etc. This information is only available through a builtin, which requires that the enclosing kernel must input that builtin and that the function add an additional input:</p>

<pre><code class="language-wgsl">@compute @workgroup_size(128) fn reduceKernel(
  @builtin(global_invocation_id) gid: vec3u,
  @builtin(local_invocation_index) lidx: u32,
  @builtin(workgroup_id) wgid: vec3u) {
  let r = reduceWorkgroup(&amp;in, gid.x);
  if (lidx == 0) {
    out[wgid.x] = r;
  }
}
</code></pre>

<p>While this second argument to <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> adds flexibility, it would be nice to not require the kernel author to plumb a builtin through the kernel if that builtin was used in the default way.</p>

<p>(Look at the use of <code class="language-plaintext highlighter-rouge">wgid.x</code> in the above kernel and recall the comment above that “It would be useful to have a 1D builtin equivalent for every 3D builtin (specifically, <code class="language-plaintext highlighter-rouge">workgroup_index</code> and <code class="language-plaintext highlighter-rouge">global_invocation_index</code>)”—here is a use for <code class="language-plaintext highlighter-rouge">workgroup_index</code>.)</p>

<h2 id="a-working-implementation">A Working Implementation</h2>

<p>Kernel (WGSL) code:</p>

<pre><code class="language-wgsl">@compute @workgroup_size(128) fn reduceKernel(
  @builtin(global_invocation_id) gid: vec3u,
  @builtin(local_invocation_index) lidx: u32,
  @builtin(workgroup_id) wgid: vec3u) {
  let r = reduceWorkgroup(&amp;in, gid.x);
  if (lidx == 0) {
    out[wgid.x] = r;
  }
}
</code></pre>

<p>Implementation of <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code>:</p>

<pre><code class="language-wgsl">var&lt;workgroup&gt; wg_temp: array&lt;atomic&lt;u32&gt;, 1&gt;;

fn reduceWorkgroup(input: ptr&lt;storage, array&lt;u32&gt;, read&gt;,
                   gid: vec3u) -&gt; u32 {
  atomicAdd(&amp;wg_temp[0], input[gid.x]);
  workgroupBarrier();
  return atomicLoad(&amp;wg_temp[0]);
}
</code></pre>

<p>Note that nothing in the actual kernel itself is specific to datatype. (Or workgroup size, for that matter.)</p>

<h3 id="generalizing-our-implementation-">Generalizing our implementation …</h3>

<h4 id="-across-datatypes">… across datatypes</h4>

<p>So, as an experiment, let’s consider adding another <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> function that reduces <code class="language-plaintext highlighter-rouge">f32</code> values rather than the <code class="language-plaintext highlighter-rouge">u32</code> values above. The callee code does not change, but the library function implementation does:</p>

<pre><code class="language-wgsl">var&lt;workgroup&gt; wg_temp: array&lt;atomic&lt;f32&gt;, 1&gt;;

fn reduceWorkgroup(input: ptr&lt;storage, array&lt;f32&gt;, read&gt;,
                   gid: vec3u) -&gt; f32 {
  atomicAdd(&amp;wg_temp[0], input[gid.x]);
  workgroupBarrier();
  return atomicLoad(&amp;wg_temp[0]);
}
</code></pre>

<p>This addition of an <code class="language-plaintext highlighter-rouge">f32 reduceWorkgroup</code> function uncovers two new issues:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">wg_temp</code> workgroup variable now has a name collision between the <code class="language-plaintext highlighter-rouge">u32</code> variant and the <code class="language-plaintext highlighter-rouge">f32</code> variant.</li>
  <li>We now have two different functions named <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code>, which is not permitted in WGSL. We can’t overload a function name. Type-specific dispatch would be useful here.</li>
</ul>

<p>The careful WGSL developer notices a third issue: there is no <code class="language-plaintext highlighter-rouge">atomicAdd</code> on <code class="language-plaintext highlighter-rouge">f32</code> variables.</p>

<p>The above issues motivate a library design that has different functions for each datatype, specifically here <code class="language-plaintext highlighter-rouge">reduceWorkgroupU32</code> and <code class="language-plaintext highlighter-rouge">reduceWorkgroupF32</code>.</p>

<h4 id="-across-storage-locations">… across storage locations</h4>

<p>Recall that our <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> signature shows that the input data is located in global (storage) memory:</p>

<pre><code class="language-wgsl">fn reduceWorkgroup(input: ptr&lt;storage, array&lt;u32&gt;, read&gt;,
                   gid: vec3u) -&gt; u32 {
  atomicAdd(&amp;wg_temp[0], input[gid.x]);
  workgroupBarrier();
  return atomicLoad(&amp;wg_temp[0]);
}
</code></pre>

<p>Let’s consider a <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> that instead stored its input in workgroup memory. It would be ideal if we could support inputs in either global (storage) or workgroup memory without any changes to the code.</p>

<pre><code class="language-wgsl">fn reduceWorkgroup(input: ptr&lt;workgroup, array&lt;u32&gt;, SIZE&gt;,
                   gid: vec3u) -&gt; u32 {
  atomicAdd(&amp;wg_temp[0], input[gid.x]);
  workgroupBarrier();
  return atomicLoad(&amp;wg_temp[0]);
}
</code></pre>

<p>Note that the <em>bodies</em> of these two functions are identical; only the argument type is different. Nonetheless WGSL requires we implement both functions separately because they have different argument types. This argues for a library designer separately implementing <code class="language-plaintext highlighter-rouge">reduceWorkgroupWGU32</code> and <code class="language-plaintext highlighter-rouge">reduceWorkgroupStorageU32</code>.</p>

<h2 id="a-more-realistic-workgroup-reduce-function">A More Realistic Workgroup Reduce Function</h2>

<p>The above functions are simple but not high-performance, because all instances must serialize on the atomic reduction variable. Higher-performance reduce implementations instead exploit parallelism across instances within a workgroup. As well, the highest-performance implementations will likely make use of WebGPU subgroups.</p>

<p>One of the challenges of subgroups is that their size is not constant within WebGPU. In fact, different kernels running within the same application on some WebGPU-capable hardware may not even share the same subgroup size. If we wish to write a subgroup-size-agnostic kernel, we must support all possible subgroup sizes within that kernel. The challenge with such an implementation is that any allocation that depends on the subgroup size must perform a worst-case allocation that works with any subgroup size.</p>

<p>WebGPU provides two adapter properties—<code class="language-plaintext highlighter-rouge">MIN_SUBGROUP_SIZE</code> and <code class="language-plaintext highlighter-rouge">MAX_SUBGROUP_SIZE</code>—that we can use to perform allocations. Below is the start of a high-performance workgroup-reduce kernel that requires a workgroup-memory allocation for partial reductions. The size of the workgroup-memory allocation is directly proportional to the workgroup size (variable: <code class="language-plaintext highlighter-rouge">{$workgroupSize}</code>) and inversely proportional to the subgroup size (variable: <code class="language-plaintext highlighter-rouge">${MIN_SUBGROUP_SIZE}</code>).</p>

<pre><code class="language-wgsl">const BLOCK_DIM: u32 = ${workgroupSize};
const TEMP_WG_MEM_SIZE = BLOCK_DIM / ${MIN_SUBGROUP_SIZE};
var&lt;workgroup&gt; wg_temp: array&lt;u32, TEMP_WG_MEM_SIZE&gt;;

fn reduceWorkgroup(input: ptr&lt;storage, array&lt;u32&gt;, read&gt;,
                   gid: vec3u, lidx: u32, sgid: u32, sgsz: u32
                   ) -&gt; u32 {
  let sid = lidx / sgsz;
  let lane_log = u32(countTrailingZeros(sgsz)); /* log_2(sgsz) */
  let local_spine: u32 = BLOCK_DIM &gt;&gt; lane_log;
  /* BLOCK_DIM / subgroup size; how many partial reductions in this tile? */
  // ...
}
</code></pre>

<p>The value of <code class="language-plaintext highlighter-rouge">MIN_SUBGROUP_SIZE</code> (and more broadly, any value that is adapter-dependent and thus not determinable until runtime) has two impacts on this code:</p>

<ul>
  <li>First, the allocation is inversely proportional to <code class="language-plaintext highlighter-rouge">MIN_SUBGROUP_SIZE</code>. We would like to only allocate the memory we need. It is not clear in WGSL what the right way to perform this specialization might be: static (compile-time) specialization, such as what a C++ template provides, or instead runtime compilation that incorporates the adapter property.</li>
  <li>Second, the algorithm can be made simpler if the subgroup sizes are large enough (specifically, if <code class="language-plaintext highlighter-rouge">MIN_SUBGROUP_SIZE * MIN_SUBGROUP_SIZE &gt;= workgroupSize</code>). It would be desirable to have the maximum possible compile-time specialization for such a decision rather than making all aspects of the decision at runtime.</li>
</ul>

<p>In summary, the presence of an adapter-specific property, only discoverable at runtime, mandates making runtime decisions that developers would prefer to do at compile time.</p>

<h2 id="multiple-implementations-are-interesting-but-builtins-complicate-function-signatures">Multiple implementations are interesting, but builtins complicate function signatures</h2>

<p>It would be desirable for a library user to write code that could call <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> but not have to choose which implementation of <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code>; instead, that choice could be made by the underlying library in some static or runtime-dependent way. This ability would also be useful for the library developer, who might want to try different <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> implementations in real workloads to determine the fastest one without having to change the function call within the workload. However, if each <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> implementation had a different function signature, this would be impossible.</p>

<p>Unfortunately, the function signatures of different implementations of <code class="language-plaintext highlighter-rouge">reduceWorkgroup</code> (above) are different.</p>

<pre><code class="language-wgsl">fn reduceWorkgroup(input: ptr&lt;storage, array&lt;u32&gt;, read&gt;,
                   gid: vec3u) -&gt; u32
</code></pre>

<pre><code class="language-wgsl">fn reduceWorkgroup(input: ptr&lt;storage, array&lt;u32&gt;, read&gt;,
                   gid: vec3u, lidx: u32, sgid: u32, sgsz: u32
                   ) -&gt; u32
</code></pre>

<p>The difference is which builtins are used. Builtins must be passed in through the function signature, and if different implementations use a different set of builtins, their function signatures will differ.</p>

<p>One remedy is to pass every builtin into every function. This is verbose and quite kludgey. Instead we have addressed this problem by …</p>

<h3 id="packing-built-ins-into-structs">Packing Built-ins into Structs</h3>

<p>To simplify the function signatures, we pack built-ins into structs:</p>

<p>Code snippet</p>

<pre><code class="language-wgsl">struct Builtins {
 @builtin(global_invocation_id) gid: vec3u,
 @builtin(num_workgroups) nwg: vec3u,
 @builtin(workgroup_id) wgid: vec3u,
 @builtin(local_invocation_index) lidx: u32,
 @builtin(local_invocation_id) lid: vec3u,
 @builtin(subgroup_size) sgsz: u32,
 @builtin(subgroup_invocation_id) sgid: u32
}
</code></pre>

<p>We also divide those builtins into two different structs, differentiated by whether the member is uniform or non-uniform. The use of <code class="language-plaintext highlighter-rouge">BuiltinsUniform</code> is sometimes necessary to ensure workgroup or subgroup uniformity.</p>

<pre><code class="language-wgsl">struct BuiltinsNonuniform {
  @builtin(global_invocation_id) gid: vec3u /* 3D thread id in compute shader grid */,
  @builtin(local_invocation_index) lidx: u32 /* 1D thread index within workgroup */,
  @builtin(local_invocation_id) lid: vec3u /* 3D thread index within workgroup */,
  @builtin(subgroup_invocation_id) sgid: u32 /* 1D thread index within subgroup */
}

struct BuiltinsUniform {
  @builtin(num_workgroups) nwg: vec3u /* == dispatch */,
  @builtin(workgroup_id) wgid: vec3u /* 3D workgroup id within compute shader grid */,
  @builtin(subgroup_size) sgsz: u32 /* subgroup size */
}
</code></pre>

<p>This allows for a cleaner function signature, but mandates that the library user must use the library’s naming conventions.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We conclude by reiterating the main pain points in WGSL that make writing generic compute libraries challenging:</p>

<ul>
  <li>Explicit declaration of @builtins.</li>
  <li>Lack of 1D variants for all built-ins.</li>
  <li>Module-scoped workgroup memory.</li>
  <li>Lack of function overloading and templating.</li>
</ul>

<p>These limitations force library designers to choose between metaprogramming to generate many specialized functions or using template literals to create a few complex kernels.</p>

  </div>

  <a class="u-url" href="/gridwise/docs/2025/04/08/writing-a-webgpu-wgsl-workgroup-reduce-function/" hidden></a>
</article>

      </div>
    </main><link id="fa-stylesheet" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css">

<footer class="site-footer h-card">
  <data class="u-url" value="/gridwise/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
      </div>
      <div class="footer-col">
        <p>WebGPU compute primitives in JavaScript</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list">
  <li>
    <a href="http://localhost:4000/gridwise/feed.xml" target="_blank" title="Subscribe to syndication feed">
      <svg class="svg-icon grey" viewbox="0 0 16 16">
        <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
          11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
          13.806c0-1.21.983-2.195 2.194-2.195zM10.606
          16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
        />
      </svg>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>

</body>

</html>
